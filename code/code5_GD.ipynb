{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd172ef-146f-4723-93d2-3b2180e56579",
   "metadata": {},
   "source": [
    "# MATH170 - Chapter *: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b63b84d-ddb5-42fc-9ece-c5619985e582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of y: 2*x - 6\n",
      "0 0.6000000000000001 10.0\n",
      "1 1.08 6.76\n",
      "2 1.464 4.6864\n",
      "3 1.7711999999999999 3.359296\n",
      "4 2.01696 2.50994944\n",
      "5 2.213568 1.9663676415999998\n",
      "6 2.3708544 1.618475290624\n",
      "7 2.49668352 1.3958241859993603\n",
      "8 2.597346816 1.2533274790395905\n",
      "9 2.6778774528 1.1621295865853378\n",
      "10 2.74230196224 1.1037629354146163\n",
      "11 2.793841569792 1.0664082786653544\n",
      "12 2.8350732558336 1.0425012983458268\n",
      "13 2.86805860466688 1.0272008309413292\n",
      "14 2.894446883733504 1.0174085318024506\n",
      "15 2.9155575069868034 1.0111414603535684\n",
      "16 2.932446005589443 1.0071305346262838\n",
      "17 2.945956804471554 1.0045635421608217\n",
      "18 2.9567654435772432 1.0029206669829258\n",
      "19 2.9654123548617948 1.0018692268690725\n",
      "20 2.9723298838894356 1.0011963051962065\n",
      "21 2.9778639071115487 1.000765635325572\n",
      "22 2.982291125689239 1.0004900066083662\n",
      "23 2.985832900551391 1.0003136042293543\n",
      "24 2.988666320441113 1.0002007067067868\n",
      "25 2.9909330563528904 1.0001284522923435\n",
      "26 2.9927464450823122 1.0000822094670998\n",
      "27 2.99419715606585 1.0000526140589439\n",
      "28 2.99535772485268 1.0000336729977242\n",
      "29 2.996286179882144 1.0000215507185435\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Gradient Descent using SymPy for derivative\n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "# 1. Define the symbolic variable and the function\n",
    "x = sp.Symbol('x', real=True)\n",
    "y_expr = (x - 3)**2 + 1\n",
    "\n",
    "# 2. Compute the derivative using SymPy\n",
    "dy_dx = sp.diff(y_expr, x)\n",
    "print(\"Derivative of y:\", dy_dx)\n",
    "\n",
    "# 3. Convert to numeric functions for iteration\n",
    "f = sp.lambdify(x, y_expr, 'numpy')\n",
    "df = sp.lambdify(x, dy_dx, 'numpy')\n",
    "\n",
    "# 4. Perform manual gradient descent iterations\n",
    "x_val = 0.0          # starting point\n",
    "eta = 0.1            # learning rate\n",
    "\n",
    "for i in range(30):\n",
    "    y_val = f(x_val)\n",
    "    grad_val = df(x_val)\n",
    "    x_val = x_val - eta * grad_val\n",
    "    print(i, x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb8d58-30db-4837-9a0d-2ae805164d0a",
   "metadata": {},
   "source": [
    "## TODO: Hands-On 1\n",
    "Task A. Change y_expr to (x - 1)**4 + 0.5.   \n",
    "Task B. Rerun the derivative and iteration steps.   \n",
    "Task C. Try two different learning rates (0.01 and 0.2) and describe what happens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226517ad-58c9-4c06-8012-c07911530562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ba08ca-f1a3-4690-a6cc-5eb2e42646c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss L(a): (1.2 - 0.5*a)**2/10 + (2.1 - 1.0*a)**2/10 + (3.7 - 1.8*a)**2/10 + (4.2 - 2.2*a)**2/10 + (6.0 - 3.0*a)**2/10 + (7.6 - 3.7*a)**2/10 + (8.4 - 4.1*a)**2/10 + (9.6 - 4.8*a)**2/10 + (10.5 - 5.2*a)**2/10 + (11.8 - 5.9*a)**2/10\n",
      "dL/da: 26.744*a - 53.892\n",
      "0 0.5389200000000001 54.315\n",
      "20 2.012182004906834 0.016161618218347388\n",
      "40 2.0151003995011494 0.015948550043379812\n",
      "60 2.01510618056859 0.015948549207302126\n",
      "80 2.0151061920203457 0.01594854920729878\n",
      "100 2.01510619204303 0.015948549207298816\n",
      "120 2.0151061920430746 0.015948549207298855\n",
      "140 2.0151061920430746 0.015948549207298855\n",
      "160 2.0151061920430746 0.015948549207298855\n",
      "180 2.0151061920430746 0.015948549207298855\n",
      "Final a: 2.0151061920430746 Final loss: 0.015948549207298855\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Linear Regression with single slope a using MSE = mean((y - a*x)**2)\n",
    "\n",
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare about ten data points\n",
    "X = np.array([0.5, 1.0, 1.8, 2.2, 3.0, 3.7, 4.1, 4.8, 5.2, 5.9], dtype=float)\n",
    "Y = np.array([1.2, 2.1, 3.7, 4.2, 6.0, 7.6, 8.4, 9.6, 10.5, 11.8], dtype=float)\n",
    "n = len(X)\n",
    "\n",
    "# 2. Build symbolic loss L(a) = (1/n) * sum (y_i - a*x_i)^2\n",
    "a = sp.Symbol('a', real=True)\n",
    "L_expr = sum((sp.Float(Y[i]) - a*sp.Float(X[i]))**2 for i in range(n)) / n\n",
    "\n",
    "# 3. Derivative with respect to a\n",
    "dL_da = sp.diff(L_expr, a)\n",
    "print(\"Loss L(a):\", L_expr)\n",
    "print(\"dL/da:\", dL_da)\n",
    "\n",
    "# 4. Convert to numeric functions\n",
    "L = sp.lambdify(a, L_expr, 'numpy')\n",
    "g = sp.lambdify(a, dL_da, 'numpy')\n",
    "\n",
    "# 5. Manual gradient descent iteration\n",
    "a_val = 0.0\n",
    "eta = 0.01\n",
    "max_iter = 200\n",
    "\n",
    "for i in range(max_iter):\n",
    "    L_val = L(a_val)\n",
    "    grad_val = g(a_val)\n",
    "    a_val = a_val - eta * grad_val\n",
    "    if i % 20 == 0:\n",
    "        print(i, a_val, L_val)\n",
    "\n",
    "print(\"Final a:\", a_val, \"Final loss:\", L(a_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757b0c7-1b37-4119-97e2-587c32545089",
   "metadata": {},
   "source": [
    "## TODO: Hands-On 2\n",
    "Task A. Use this new dataset  \n",
    "Task B. Build L(a) = mean((y - a*x)**2)  \n",
    "Task C. Run gradient descent with eta = 0.005 and eta = 0.05, compare convergence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3886f580-cace-4055-ad4d-7a17fd896eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([0.2, 0.9, 1.3, 1.9, 2.6, 3.1, 3.9, 4.3, 5.0, 5.7], dtype=float)\n",
    "Y = np.array([0.6, 1.7, 2.4, 3.1, 4.2, 5.1, 6.0, 6.7, 7.9, 9.0], dtype=float)\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "136dd3bc-cf5f-4066-8c86-ba2972f44535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal a: [2.01510616]\n",
      "Minimum loss: 0.015948549207311764\n"
     ]
    }
   ],
   "source": [
    " # Part 3: Using scipy.optimize to minimize L(a) = mean((y - a*x)**2)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar, minimize\n",
    "\n",
    "# 1. Same dataset as before\n",
    "X = np.array([0.5, 1.0, 1.8, 2.2, 3.0, 3.7, 4.1, 4.8, 5.2, 5.9], dtype=float)\n",
    "Y = np.array([1.2, 2.1, 3.7, 4.2, 6.0, 7.6, 8.4, 9.6, 10.5, 11.8], dtype=float)\n",
    "n = len(X)\n",
    "\n",
    "# 2. Define the loss function numerically\n",
    "def loss(a):\n",
    "    pred = a * X\n",
    "    return np.mean((Y - pred)**2)   # (truth - prediction)^2\n",
    "\n",
    "# 3. Minimize using scipy\n",
    "# result = minimize_scalar(loss)\n",
    "result = minimize(loss, x0=np.array([0]), method='BFGS')\n",
    "print(\"Optimal a:\", result.x)\n",
    "print(\"Minimum loss:\", result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "782fdce4-ed23-4db8-be93-95a6d819d284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal a: 1.992209325064098\n",
      "Optimal b: 0.09508597009443792\n",
      "Minimum loss: 0.013917698761487917\n"
     ]
    }
   ],
   "source": [
    "# Part 3 (extended): Two-parameter linear regression using scipy.optimize.minimize\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 1. Same dataset as before\n",
    "X = np.array([0.5, 1.0, 1.8, 2.2, 3.0, 3.7, 4.1, 4.8, 5.2, 5.9], dtype=float)\n",
    "Y = np.array([1.2, 2.1, 3.7, 4.2, 6.0, 7.6, 8.4, 9.6, 10.5, 11.8], dtype=float)\n",
    "n = len(X)\n",
    "\n",
    "# 2. Define the loss function for two parameters (a, b)\n",
    "def loss(params):\n",
    "    a, b = params\n",
    "    pred = a * X + b\n",
    "    return np.mean((Y - pred)**2)   # (truth - prediction)^2\n",
    "\n",
    "# 3. Use SciPy minimize\n",
    "result = minimize(loss, x0=np.array([0.0, 0.0]), method='BFGS')\n",
    "\n",
    "# 4. Display results\n",
    "a_opt, b_opt = result.x\n",
    "print(\"Optimal a:\", a_opt)\n",
    "print(\"Optimal b:\", b_opt)\n",
    "print(\"Minimum loss:\", result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4aa48-05f0-4dae-8588-6be99820ea99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
